{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spark check\n",
    "import findspark\n",
    "findspark.init()\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark import SparkConf\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext, HiveContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"my_app\")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking timestamp comparison for spark dataframe\n",
    "\n",
    "prev_row={}\n",
    "pixel={}\n",
    "cur_row={}\n",
    "prev_row['performance_timeobj'] = str(datetime.strptime('2017-02-01 23:40:00.0', \"%Y-%m-%d %H:%M:%S.0\"))\n",
    "pixel['order_timestamp'] = datetime.strptime('2017-02-02 13:37:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "cur_row['performance_timeobj'] = datetime.strptime('2017-02-02 00:36:34.0', \"%Y-%m-%d %H:%M:%S.0\")\n",
    "delta = -timedelta(hours=16)\n",
    "print prev_row['performance_timeobj'], cur_row['performance_timeobj']\n",
    "print type(prev_row['performance_timeobj']), type(cur_row['performance_timeobj'])\n",
    "# if (prev_row['performance_timeobj'] + delta)  < pixel['order_timestamp'] <= (cur_row['performance_timeobj'] + delta):\n",
    "#     print 'yipee'\n",
    "# else:\n",
    "#     print 'noo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark datetime data types\n",
    "\n",
    "a = '2017-02-01 23:40:00'\n",
    "da = datetime.strptime('2017-02-01 23:40:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "df = sqlContext.createDataFrame([(da,da),(da,da)],['dt1','dt2'])\n",
    "df.show()\n",
    "print df.take(1)[0], type(df.take(1)[0]['dt1']) \n",
    "print df.printSchema()\n",
    "\n",
    "#udf_timestamp = udf(lambda x : datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"), TimestampType())\n",
    "udf_timestamp = udf(lambda x : x, TimestampType())\n",
    "df = df.withColumn(\"dt1\", udf_timestamp(\"dt1\"))\n",
    "print df.printSchema()\n",
    "print df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '2017-02-01 23:40:00'\n",
    "#print TimestampType\n",
    "\n",
    "from pyspark.sql import Row\n",
    "zz = Row(b=1) + Row(a=2)\n",
    "print type(zz)\n",
    "print sc.parallelize([zz]).toDF(['b','a']).show()\n",
    "#zz2 = Row(b=1)\n",
    "#print zz2.toDF(['b'])\n",
    "z = Row(b=2)\n",
    "print z['b']\n",
    "\n",
    "zz = Row(b=1)\n",
    "print zz, type(zz)\n",
    "print zz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = '2017-02-01 23:40:00'\n",
    "b = '2017-02-02 23:40:00'\n",
    "at = pd.to_datetime(a)\n",
    "bt = pd.to_datetime(b)\n",
    "print pd.to_datetime(a), type(pd.to_datetime(a))\n",
    "print at > bt\n",
    "print a, at\n",
    "print dir(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark logging\n",
    "# print sc.applicationId() #sample tracking url : http://yn0004.dev.abc:8088/proxy/application_1483331339927_2935/\n",
    "# print sc._jsc.sc().uiWebUrl().get()\n",
    "# log4jLogger = sc._jvm.org.apache.log4j\n",
    "# LOGGER = log4jLogger.LogManager.getLogger(__name__)\n",
    "# LOGGER.info(\"pyspark script logger initialized\")\n",
    "\n",
    "#Pyspark logging\n",
    "# print sc._jsc.sc().uiWebUrl().get()\n",
    "# log4jLogger = sc._jvm.org.apache.log4j\n",
    "# LOGGER = log4jLogger.LogManager.getLogger(__name__)\n",
    "# LOGGER.info(\"pyspark script logger initialized\")\n",
    "\n",
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Experimenting with dataframe partitioning\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "b=sqlContext.createDataFrame(sc.parallelize([(1,1,1,10), (1,1,1,2), (1,1,2,2), (2,1,2,2), (2,1,2,1), (1,2,1,1), (1,2,1,2), (1,1,2,1) ]),['a','b','c','d'])\n",
    "#b.repartition('a','b','c').orderBy(['a','b','c']).show()\n",
    "#b.repartition('c').show()\n",
    "# print b.show()\n",
    "# print b.repartition('a').show()\n",
    "# print b.repartition('a').rdd.getNumPartitions()\n",
    "# print b.repartition('a').sortWithinPartitions('d').show()\n",
    "# print b.repartition('a','b','c').show()\n",
    "# print b.repartition('a','b','c').rdd.getNumPartitions()\n",
    "# print b.repartition('a','b','c').rdd.glom().collect()\n",
    "#print b.orderBy(['c','b','a']).show()\n",
    "#print b.orderBy(['c','b','a']).rdd.getNumPartitions()\n",
    "#repartition and orderby are not required at same time as order is same as sort and will reshuffle\n",
    "#print b.repartition('a','b','c').orderBy(['c','b','a']).show()\n",
    "#print b.repartition('a','b','c').orderBy(['c','b','a']).rdd.getNumPartitions()\n",
    "# tmp = b.orderBy(['c','b','a']).sortWithinPartitions('d')\n",
    "# print b.orderBy(['c','b','a']).rdd.getNumPartitions()\n",
    "# print b.orderBy(['c','b','a']).rdd.glom().collect()\n",
    "# print b.orderBy(['c','b','a']).sortWithinPartitions('d').show()\n",
    "print 'check:'\n",
    "#tmp = b.orderBy(['c','b']).sortWithinPartitions(['a','d'])\n",
    "#tmp = b.orderBy(['a']).sortWithinPartitions(['a','b','c','d'])\n",
    "tmp = b.orderBy(['c']).sortWithinPartitions(['c','b','a','d'],ascending=[True,True,True,False])\n",
    "print tmp.show()\n",
    "print tmp.rdd.getNumPartitions()\n",
    "print tmp.rdd.glom().collect()\n",
    "#print b.orderBy(['c','b','a']).sortWithinPartitions('d').rdd.getNumPartitions()\n",
    "#print b.orderBy(['c','b','a']).sort('d').show()\n",
    "#print b.orderBy(['c','b','a']).sort('d').rdd.getNumPartitions()\n",
    "\n",
    "#http://stackoverflow.com/questions/33831561/pyspark-repartition-vs-partitionby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimenting with rdd partitioning\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "b=sqlContext.createDataFrame(sc.parallelize([(1,1,1,1), (1,1,1,2), (1,1,2,2), (1,1,2,1), (2,1,2,2), (2,1,2,1), (1,2,1,1), (1,2,1,2) ]),['a','b','c','d'])\n",
    "\n",
    "print ord_test_df.rdd.map(lambda x : (x['p_performance_campaignId'], x)).partitionBy(840).distinct().map(lambda x : x[0]).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dataframe creation\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "a = {'q':1,'b':2,'d':12}\n",
    "b = {'qq':1,'b':2,'c':3}\n",
    "b=sqlContext.createDataFrame(sc.parallelize([a,b]))\n",
    "c = b.withColumn('ab',b['b']+2)\n",
    "d = c.withColumn('ab',c['ab']+1)\n",
    "print d.show()\n",
    "# c = sc.parallelize([a,b])\n",
    "# print type(c), dir(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time zone\n",
    "#http://stackoverflow.com/questions/18812638/get-timezone-used-by-datetime-datetime-fromtimestamp\n",
    "#http://stackoverflow.com/questions/3168096/getting-computers-utc-offset-in-python\n",
    "print dir(datetime)\n",
    "print datetime.fromtimestamp(float(1483540473910)/1000) #Gives diff result on diff machines\n",
    "print datetime.utcfromtimestamp(float(1483540473910)/1000), datetime.utcfromtimestamp(float(1483540473910)/1000) - timedelta(hours=8) \n",
    "#do = datetime.fromtimestamp(float(1483540473910)/1000)\n",
    "#print datetime.utcoffset(do)\n",
    "a, b = 1.481702427E9, 1.481673627E9\n",
    "print datetime.fromtimestamp(a), datetime.fromtimestamp(b) \n",
    "print datetime.utcfromtimestamp(a), datetime.utcfromtimestamp(b)\n",
    "##Result on local laptop\n",
    "# 2016-12-14 00:00:27 2016-12-13 16:00:27\n",
    "# 2016-12-14 08:00:27 2016-12-14 00:00:27\n",
    "##Below is result on some server\n",
    "# >>> a, b = 1.481702427E9, 1.481673627E9\n",
    "# >>> print datetime.fromtimestamp(a), datetime.fromtimestamp(b) \n",
    "# 2016-12-14 08:00:27 2016-12-14 00:00:27\n",
    "# >>> print datetime.utcfromtimestamp(a), datetime.utcfromtimestamp(b)\n",
    "# 2016-12-14 08:00:27 2016-12-14 00:00:27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access hdfs avro data\n",
    "# run this when in JupyterHub\n",
    "import os\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark/')\n",
    "\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext \n",
    "from pyspark.sql import HiveContext\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.sql(\"CREATE TEMPORARY TABLE table_name USING com.databricks.spark.avro OPTIONS (path 'hdfs:///user/hadoop/a/b.avro')\")\n",
    "\n",
    "df = sqlContext.sql(\"SELECT a,b,c FROM table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Window function links\n",
    "https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n",
    "http://stackoverflow.com/questions/38414768/pyspark-data-frame-api-filter-rows-where-the-lead-lag-are-specific-values-wind\n",
    "http://stackoverflow.com/questions/33207164/spark-window-functions-rangebetween-dates\n",
    "http://stackoverflow.com/questions/36725353/applying-a-window-function-to-calculate-differences-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Window functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, sum, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import HiveContext\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "hiveContext = HiveContext(sc)\n",
    "\n",
    "customers = sc.parallelize([(1234, 1234, datetime.strptime('2016-02-02 00:01:12', \"%Y-%m-%d %H:%M:%S\"), 1, 110, 1),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 02:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 111, 2),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 03:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 3),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 01:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 5),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 02:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 113, 10),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 01:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 114, 14),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 03:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 115, 22)])\n",
    "df = hiveContext.createDataFrame(customers, [\"performance_campaignId\", \"performance_keywordId\", \"performance_timeobj\", \"bq_resale_orders\", \"bq_resale_gtv\", \"performance_cost\"])\n",
    "print df.printSchema()\n",
    "\n",
    "partitioning_list = [\"performance_campaignId\",\"performance_keywordId\"]\n",
    "\n",
    "#Moving average\n",
    "wSpec1 = Window.partitionBy(partitioning_list).orderBy(\"performance_timeobj\").rowsBetween(-1, 0)\n",
    "print df.withColumn(\"movingAvg\", avg(df[\"bq_resale_gtv\"]).over(wSpec1)).show()\n",
    "\n",
    "#cumulative sum\n",
    "wSpec2 = Window.partitionBy(partitioning_list).orderBy(\"performance_timeobj\").rowsBetween(-sys.maxint, 0) #Figure out Long.MinValue\n",
    "print df.withColumn(\"cumulativeSum\", sum(df[\"bq_resale_orders\"]).over(wSpec2)).show()\n",
    "\n",
    "#Filtered date windowed query\n",
    "wSpec3 = Window.partitionBy(partitioning_list).orderBy(col(\"performance_timeobj\").cast(\"timestamp\").cast(\"long\")).rangeBetween(-1 * 3600, 0)\n",
    "print df.withColumn(\"cumulativeDateFilteredSum\", sum(df[\"bq_resale_orders\"]).over(wSpec3)).show()\n",
    "\n",
    "#Testing lag\n",
    "wSpec4 = Window.partitionBy(partitioning_list).orderBy(col(\"performance_timeobj\").cast(\"timestamp\").cast(\"long\")).rangeBetween(-1 * 3600, 0)\n",
    "#print df.withColumn(\"lagCheck\", F.first(df[\"performance_cost\"]).over(wSpec4)).show()\n",
    "print df.withColumn(\"lagCheck\", F.first(df[\"performance_cost\"]).over(wSpec4) - df[\"performance_cost\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test window query out of range values\n",
    "\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "#Filtered date windowed query\n",
    "wSpec3 = Window.partitionBy(partitioning_list).orderBy(col(\"performance_timeobj\").cast(\"timestamp\").cast(\"long\")).rangeBetween(-1 * 3600, -1)\n",
    "df2 = df.withColumn(\"cumulativeDateFilteredSum\", coalesce(df[\"bq_resale_orders\"] - sum(df[\"bq_resale_orders\"]).over(wSpec3),df[\"bq_resale_orders\"]))\n",
    "print df2.show()\n",
    "df2.repartition(1).write.format('json').save('/tmp/tmp.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column coalesce to replace null values\n",
    "# http://stackoverflow.com/questions/42990533/pyspark-replace-null-in-column-with-value-in-other-column\n",
    "\n",
    "from pyspark.sql.functions import coalesce\n",
    "cDf = sqlContext.createDataFrame([(None, None), (1, None), (None, 2),(3,4)], (\"a\", \"b\"))\n",
    "cDf.show()\n",
    "print cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print -sys.maxint\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "d = datetime.strptime('2016-02-02 01:02:12', \"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "print d - timedelta(days=7)\n",
    "print d - timedelta(days=1)\n",
    "print d - timedelta(hours=1), type(d - timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark mapparitions tinkering\n",
    "\n",
    "def my_print_f(iterator):\n",
    "    if iterator:\n",
    "        print 'yo:'\n",
    "        for r in iterator:\n",
    "            print r\n",
    "    print '****'\n",
    "    \n",
    "def my_print_f(x):\n",
    "    print 'yo:'\n",
    "    print r\n",
    "    print '****'\n",
    "\n",
    "d = sc.parallelize([('ab',1,10),('ab',2,11),('qb',3,4),('qb',5,7),('a',1,1),('a',0,2)],3)\n",
    "#d = sc.parallelize([1,2,3,3,4,5,6,6,7,8,8,9],3)\n",
    "df = sqlContext.createDataFrame(d,['grp','order','val'])\n",
    "df = df.orderBy(['grp']).sortWithinPartitions(\"order\")\n",
    "print df.rdd.getNumPartitions()\n",
    "#print df.show()\n",
    "#df.rdd.foreachPartition(print_f)\n",
    "#print d.getNumPartitions()\n",
    "#print d.collect()\n",
    "# print df.rdd.mapPartitions(my_print_f, preservesPartitioning=True).collect()\n",
    "#print df.rdd.foreachPartition(my_print_f)\n",
    "# pixel_merged_rdd = ord_df.rdd.mapPartitions(process_keyword_partition, preservesPartitioning=True)\n",
    "\n",
    "print df.rdd.map(my_print_f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark window query based join\n",
    "\n",
    "customers = sc.parallelize([(1234, 1234, datetime.strptime('2016-02-02 00:01:12', \"%Y-%m-%d %H:%M:%S\"), 1, 110, 1),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 02:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 111, 2),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 03:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 3),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 01:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 5),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 02:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 113, 10),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 01:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 114, 14),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 03:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 115, 22)])\n",
    "\n",
    "customers2 = sc.parallelize([(1234, 1234, datetime.strptime('2016-02-02 00:02:12', \"%Y-%m-%d %H:%M:%S\"), 1, 110, 1),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 00:03:12', \"%Y-%m-%d %H:%M:%S\"), 1, 111, 2),\n",
    "                            (1234, 1234, datetime.strptime('2016-02-02 02:03:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 3),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 01:03:12', \"%Y-%m-%d %H:%M:%S\"), 1, 112, 5),\n",
    "                            (1235, 1235, datetime.strptime('2016-02-02 02:01:12', \"%Y-%m-%d %H:%M:%S\"), 1, 113, 10),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 01:03:12', \"%Y-%m-%d %H:%M:%S\"), 1, 114, 14),\n",
    "                            (1236, 1236, datetime.strptime('2016-02-02 03:01:12', \"%Y-%m-%d %H:%M:%S\"), 1, 115, 22)])\n",
    "\n",
    "df = hiveContext.createDataFrame(customers, [\"performance_campaignId\", \"performance_keywordId\", \"performance_timeobj\", \"bq_resale_orders\", \"bq_resale_gtv\", \"performance_cost\"])\n",
    "df2 = hiveContext.createDataFrame(customers2, [\"performance_campaignId\", \"performance_keywordId\", \"performance_timeobj\", \"bq_resale_orders\", \"bq_resale_gtv\", \"performance_cost\"])\n",
    "\n",
    "print df.printSchema()\n",
    "\n",
    "partitioning_list = [\"performance_campaignId\",\"performance_keywordId\"]\n",
    "\n",
    "wSpec5 = Window.partitionBy(partitioning_list).orderBy(col(\"performance_timeobj\"))\n",
    "wSpec6 = Window.partitionBy(partitioning_list).orderBy(col(\"performance_timeobj\").cast(\"timestamp\").cast(\"long\")).rangeBetween(-1 * 3600, 0)\n",
    "df = df.withColumn(\"timestamplong\", df['performance_timeobj'].cast(\"timestamp\").cast(\"long\"))\n",
    "df2 = df2.withColumn(\"timestamplong\", df2['performance_timeobj'].cast(\"timestamp\").cast(\"long\"))\n",
    "df = df.withColumn(\"lagCheck\", F.lag(df[\"timestamplong\"]).over(wSpec5))\n",
    "#df = df.withColumn(\"aggVal\",F.sum(df2[\"bq_resale_orders\"]).over(Window.partitionBy(partitioning_list).orderBy(\"timestamplong\").rangeBetween(-1 * df['lagCheck'], 0)))\n",
    "\n",
    "df.rdd.map(my_agg(df2))\n",
    "\n",
    "print df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check partition lengths skew\n",
    "#http://stackoverflow.com/questions/28687149/how-to-get-the-number-of-elements-in-partition\n",
    "\n",
    "num_partitions = 20000\n",
    "a = sc.parallelize(range(int(1e6)), num_partitions)\n",
    "l = a.glom().map(len).collect()  # get length of each partition\n",
    "print(min(l), max(l), sum(l)/len(l), len(l))  # check if skewed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test reduceByKey\n",
    "from operator import add\n",
    "def reduce_func(accum_x, x):\n",
    "    if accum_x is None:\n",
    "        print 'accum_x none, x:', x\n",
    "    if x is None:\n",
    "        print 'x none, accum_x:', accum_x\n",
    "    if accum_x is None:\n",
    "        accum_x = x\n",
    "    else:     \n",
    "        accum_x = map(add,accum_x,x)\n",
    "    return accum_x\n",
    "\n",
    "b = sc.parallelize([(1,1,1,10), (3,1,1,2), (1,1,2,2), (2,1,2,2), (2,1,2,1), (1,2,1,1), (1,2,1,2), (3,1,2,1) ])\n",
    "#print b.map(lambda x : (x[0],x)).collect()\n",
    "#print b.map(lambda x : (x[0],x[0])).groupByKey().reduceByKey(reduce_func).collect()\n",
    "print b.map(lambda x : (x[0],x)).reduceByKey(reduce_func).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Element wise addition\n",
    "from operator import add\n",
    "a=[1,2]\n",
    "b=[2,3]\n",
    "print map(add,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining 2 dataframes\n",
    "customers1 = sc.parallelize([(1234, 2, 10, 110, 1),\n",
    "                            (1235, 1234, 1, 111, 2),\n",
    "                            (1236, 1234, 1, 112, 3),\n",
    "                            (1237, 1235, 1, 112, 5),\n",
    "                            (1238, 1235, 1, 113, 10),\n",
    "                            (1239, 1236, 1, 114, 14),\n",
    "                            (1240, 7, 11, 115, 22)])\n",
    "\n",
    "customers2 = sc.parallelize([(1234, 2, 1, 110, 1),\n",
    "                            (1233, 3, 1, 111, 2),\n",
    "                            (1232, 4, 1, 112, 3),\n",
    "                            (1231, 5, 1, 112, 5),\n",
    "                            (1237, 6, 1, 113, 10),\n",
    "                            (1240, 7, 1, 114, 14),\n",
    "                            (1290, 8, 1, 115, 22)])\n",
    "\n",
    "df1 = sqlContext.createDataFrame(customers1, [\"a1\",\"b1\",\"c1\",\"d1\",\"e1\"])\n",
    "df2 = sqlContext.createDataFrame(customers2, [\"a2\",\"b2\",\"c2\",\"d2\",\"e2\"])\n",
    "\n",
    "#Approach1\n",
    "df3 = df1.join(df2, (df1.a1 == df2.a2) & (df1.b1 == df2.b2))\n",
    "print df3.show()\n",
    "\n",
    "#Approach2\n",
    "df1.registerTempTable(\"df1\")\n",
    "df2.registerTempTable(\"df2\")\n",
    "print sqlContext.sql(\"SELECT *  from df1, df2 where df1.a1 = df2.a2 AND df1.b1 = df2.b2\").show()\n",
    "\n",
    "# #Approach1 variant\n",
    "# join_cond = (df1.a1 == df2.a2) & (df1.b1 == df2.b2)\n",
    "# print join_cond\n",
    "# df4 = df1.join(df2, join_cond, 'left_outer')\n",
    "# join_cond = (df4.a1 == df2.a2) & (df4.b1 == df2.b2)\n",
    "# df5 = df4.join(df2, join_cond, 'left_outer')\n",
    "# print df5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark reduce_by hack\n",
    "\n",
    "#https://codereview.stackexchange.com/questions/115082/generic-reduceby-or-groupby-aggregate-functionality-with-spark-dataframe\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql import DataFrame\n",
    "from collections import OrderedDict\n",
    "\n",
    "def reduce_by(self, by, cols, f, schema=None):\n",
    "    \"\"\"\n",
    "    :param self DataFrame\n",
    "    :param by a list of grouping columns \n",
    "    :param cols a list of columns to aggregate\n",
    "    :param aggregation function Row => Row\n",
    "    :return DataFrame\n",
    "    \"\"\"\n",
    "    def merge_kv(kv):\n",
    "        key, value = kv\n",
    "        return Row(**OrderedDict(zip(\n",
    "            key.__fields__ + value.__fields__, key + value)\n",
    "        ))\n",
    "\n",
    "    return (self\n",
    "        .select(struct(*by), struct(*cols))\n",
    "        .rdd\n",
    "        .reduceByKey(f)\n",
    "        .map(merge_kv)\n",
    "        .toDF(schema))\n",
    "\n",
    "DataFrame.reduce_by = reduce_by  # A quick monkey patch\n",
    "\n",
    "def foo(row1, row2):\n",
    "    \"\"\" A dummy function\n",
    "    >>> foo(Row(x=1, y=None), Row(x=None, y=2))\n",
    "    Row(x=1, y=2)\n",
    "    \"\"\"\n",
    "    return Row(**OrderedDict(zip(\n",
    "      row1.__fields__, (x if x else y for (x, y) in zip(row1, row2))\n",
    "    )))\n",
    "\n",
    "# Example data\n",
    "df = sc.parallelize([\n",
    "    (\"a\", None, 1), (\"a\", None, 2), (\"a\", 3, None),\n",
    "    (\"b\", None, 2), (\"b\", None, None), (\"c\", 2, -2), (\"c\", 1, -1)\n",
    "]).toDF([\"k\", \"v1\", \"v2\"])\n",
    "\n",
    "df.reduce_by(by=[\"k\"], cols=[\"v1\", \"v2\"], f=foo).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating dataframe column names\n",
    "\n",
    "#https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark\n",
    "\n",
    "oldColumns = df.schema.names\n",
    "newColumns = map(lambda x : 'abc_' + x, oldColumns)\n",
    "#df.schema.names = newColumns\n",
    "print oldColumns, newColumns\n",
    "\n",
    "df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), df)\n",
    "df.printSchema\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf on multiple columns\n",
    "\n",
    "customers1 = sc.parallelize([(1234, 2, 10, 110, 1),\n",
    "                            (1235, 1234, 1, 111, 2),\n",
    "                            (1236, 1234, 1, 112, 3),\n",
    "                            (1237, 1235, 1, 112, 5),\n",
    "                            (1238, 1235, 1, 113, 10),\n",
    "                            (1239, 1236, 1, 114, 14),\n",
    "                            (1240, 7, 11, 115, 22)])\n",
    "\n",
    "df1 = sqlContext.createDataFrame(customers1, [\"a1\",\"b1\",\"c1\",\"d1\",\"e1\"])\n",
    "\n",
    "udf_add = udf(lambda x,y : x+y, IntegerType())\n",
    "\n",
    "df2 = df1\\\n",
    "    .withColumn(\"f1\", udf_add(col(\"a1\"),col(\"b1\")))\n",
    "    \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect read from s3\n",
    "\n",
    "file_regex='s3://a/b*.csv'\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "# full_df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').option(\"escape\",\"\\\"\").load(file_regex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]\n",
    "b = [3,4]\n",
    "a+b\n",
    "print a+b, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model error metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): #Make sure no values are 0\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mean_ape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return mean_ape\n",
    "\n",
    "def median_absolute_percentage_error(y_true, y_pred): #Make sure no values are 0\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    median_ape = np.median(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return median_ape\n",
    "\n",
    "def compute_rmse(y_true, y_pred): \n",
    "    cur_rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return cur_rmse\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, cur_scenario, sample_size=0, predictor_size=0):\n",
    "    y_pred = map(lambda x : float(x), y_pred)\n",
    "    cur_mean_ape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    cur_median_ape = median_absolute_percentage_error(y_true, y_pred)\n",
    "    cur_rmse = compute_rmse(y_true, y_pred)\n",
    "    cur_mae = mean_absolute_error(y_true, y_pred)\n",
    "    cur_r2_score = r2_score(y_true, y_pred)\n",
    "    adjusted_r_squared = 1 - (1-cur_r2_score)*(sample_size-1)/(sample_size-predictor_size-1)\n",
    "    print 'Mean abs percent err | RMSE | Median abs percent err | Mean abs err | R2 score | Adjusted R2 score | Model |'\n",
    "    print '%s | %s | %s | %s | %s |%s | %s |'%(cur_mean_ape, cur_rmse, cur_median_ape, cur_mae, cur_r2_score, adjusted_r_squared, cur_scenario)\n",
    "    return cur_median_ape, cur_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression\n",
    "# https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/regression/RandomForestRegressor.scala\n",
    "# http://blog.learningtree.com/how-to-predict-outcomes-using-random-forests-and-spark/\n",
    "# https://spark.apache.org/docs/latest/mllib-data-types.html\n",
    "# https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2485090270202665/2400794110066361/8589256059752547/latest.html\n",
    "#https://spark.apache.org/docs/latest/ml-features.html (Use Vector assembler to group diff features into 1 group if required)\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "# categorical_cols = ['a']\n",
    "# numerical_cols = ['b','c','label']\n",
    "# categorical_fields = [StructField(field_name, StringType(), True) for field_name in categorical_cols]\n",
    "# numerical_fields = [StructField(field_name, DoubleType(), True) for field_name in numerical_cols]\n",
    "# all_fields = categorical_fields + numerical_fields\n",
    "# schema = StructType(all_fields)\n",
    "# df = sqlContext.createDataFrame(rdd, schema)\n",
    "\n",
    "#Spark ml RandomForest Regressor\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "def parse_ml(l):\n",
    "    return (l[3], Vectors.dense(l[1:3]))\n",
    "\n",
    "cur_rdd = sc.parallelize([(\"a\",1.0,1.0,10.2), (\"b\",-1.0,1.0,2.0), (\"a\",1.0,2.0,22.5), (\"b\",-5.0,2.0,2.0), (\"b\",-2.0,2.0,1.0), (\"b\",-22.0,1.0,1.9), (\"a\",2.0,1.0,12.2), (\"b\",-11.0,2.0,1.0) ])\n",
    "ml_rdd = cur_rdd.map(parse_ml)\n",
    "new_df = sqlContext.createDataFrame(ml_rdd, [\"label\", \"features\"])\n",
    "\n",
    "# Train a RandomForest model\n",
    "rf = RandomForestRegressor(numTrees=4, maxDepth=10)\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "# Train model.  This also runs the (indexer) other stages\n",
    "model = rf.fit(new_df) #model = pipeline.fit(new_df). Ignore pipeline for now for .featureImportances to work\n",
    "# Make predictions\n",
    "Y_predictions = model.transform(new_df)\n",
    "\n",
    "y_pred = Y_predictions.select('prediction').toPandas().values\n",
    "y_true = np.array([10.2,2.0,22.5,2.0,1.0,1.9,12.2,1.0])\n",
    "evaluate_predictions(y_true, y_pred, 'Spark ML RFR')\n",
    "\n",
    "print model.featureImportances\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Spark mllib RandomForest Regressor\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "cat_num_map = {\"a\":0, \"b\":1}\n",
    "\n",
    "from pyspark.mllib.feature import LabeledPoint\n",
    "def parse_mllib(l):\n",
    "    return LabeledPoint(l[3], l[1:3]) #list(cat_num_map[l[0]]) + l[1:3])\n",
    "\n",
    "cur_rdd = sc.parallelize([(\"a\",1,1.0,10.2), (\"b\",-1.0,1.0,2.0), (\"a\",1.0,2.0,22.5), (\"b\",-5.0,2.0,2.0), (\"b\",-2.0,2.0,1.0), (\"b\",-22.0,1.0,1.9), (\"a\",2.0,1.0,12.2), (\"b\",-11.0,2.0,1.0) ])\n",
    "mllib_rdd = cur_rdd.map(parse_mllib) #label, features (implicit headers)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Note: Use larger numTrees in practice.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainRegressor(mllib_rdd, categoricalFeaturesInfo={},\n",
    "                                    numTrees=3, featureSubsetStrategy=\"auto\",\n",
    "                                    impurity='variance', maxDepth=4, maxBins=32)\n",
    "\n",
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(mllib_rdd.map(lambda x : x.features))\n",
    "print(model.toDebugString())\n",
    "\n",
    "y_pred = np.array(predictions.collect())\n",
    "y_true = np.array(mllib_rdd.map(lambda x : x.label).collect())\n",
    "evaluate_predictions(y_true, y_pred, 'Spark MLLib RFR')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
