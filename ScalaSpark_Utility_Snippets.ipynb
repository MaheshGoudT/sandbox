{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker related information\n",
    "\n",
    "https://github.com/jupyter/docker-stacks\n",
    "\n",
    "localnotebookpath='/Users/maheshgoud/Documents/Git_Modules/gh/sandbox/'\n",
    "\n",
    "localdatapath='/Users/maheshgoud/Downloads/data/'\n",
    "\n",
    "#allsparkpackagenames='org.apache.spark:spark-avro_2.11:2.4.0,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2'\n",
    "\n",
    "docker run -it --rm -p 8890:8888 -v $localnotebookpath:/home/jovyan -v $localdatapath:/home/jovyan/data -e JUPYTER_ENABLE_LAB=yes -e SPARK_OPTS='--packages org.apache.spark:spark-avro_2.11:2.4.0,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2' \\\n",
    "-e PYSPARK_SUBMIT_ARGS='--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell' jupyter/all-spark-notebook:7d427e7a4dde start-notebook.sh --NotebookApp.token=''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build.sbt\n",
    "\n",
    "name := \"app\"\n",
    "version := \"1\"\n",
    "scalaVersion := \"2.11.12\"\n",
    "resolvers += \"Spark Packages Repo\" at \"http://dl.bintray.com/spark-packages/maven\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.4.0\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\"\n",
    "//libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"2.4.0\"\n",
    "//libraryDependencies += \"com.databricks\" %% \"spark-avro\" % \"4.0.0\"\n",
    "//libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.5\" % \"test\"\n",
    "//libraryDependencies += \"com.github.mrpowers\" % \"spark-fast-tests_2.11\" % \"0.12.5\" % \"test\"\n",
    "//libraryDependencies += \"mrpowers\" % \"spark-daria\" % \"2.2.0_0.12.0\" % \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful code snippets\n",
    "\n",
    "- https://www.programcreek.com/scala/org.apache.spark.SparkConf (code snippets)\n",
    "- https://gist.github.com/eddies/f37d696567f15b33029277ee9084c4a0 (s3 access, packages related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.Seq\n",
    "import sys.process._\n",
    "\n",
    "// import spark.implicits._\n",
    "// import org.apache.spark.sql.Row\n",
    "\n",
    "println(\"Spark version: \"+sc.version)\n",
    "println(\"Scala version: \"+util.Properties.versionString)\n",
    "\"python --version\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tinker with functional pattern + daily agg window function\n",
    "\n",
    "import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    "import org.apache.spark.sql.functions.{col, when}\n",
    "val F = org.apache.spark.sql.functions\n",
    "\n",
    "def distinctElemCount = F.udf( (a: Seq[Any]) => a.toSet.size )\n",
    "//def distinctElemCount = F.udf( (a: Seq[Any]) => when(a.toSet.isEmpty(), null).otherwise(a.toSet.size) )\n",
    "\n",
    "val df1 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (3, 3, 33, \"ord7\", \"b\"),    \n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "print(df1.show())\n",
    "val w = Window.partitionBy(col(\"IP\")).orderBy(\"DAY\").rangeBetween(1, 4)\n",
    "\n",
    "val df2 = df1.groupBy(col(\"IP\"), col(\"DAY\")).agg(F.sum(col(\"VAL\")).alias(\"DAILY_VAL\"), F.collect_set(col(\"ORD\")).alias(\"DAILY_ORD\")).toDF()\n",
    "print(df2.show())\n",
    "\n",
    "// compute collect set over daily collect set aggs\n",
    "val w2 = Window.orderBy(\"DAY\").rangeBetween(-5, 0)\n",
    "\n",
    "print(\"debug\")\n",
    "print(df2.withColumn(\"z1\", F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2))).withColumn(\"z2\", distinctElemCount(col(\"z1\"))).show(10, false))\n",
    "print(df2.withColumn(\"z1\", distinctElemCount(F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2)))).show())\n",
    "\n",
    "val df3 = df2.withColumn(\"WINDOWED_VAL\", F.sum(col(\"DAILY_VAL\")).over(w))\n",
    "print(df3.show())\n",
    "\n",
    "val df4 = df3.withColumn(\"ORD\", F.explode(col(\"DAILY_ORD\"))).drop(\"DAILY_ORD\")\n",
    "print(df4.show())\n",
    "\n",
    "\n",
    "println(\"------------------------------------------\")\n",
    "println(\"debug null\")\n",
    "val tmpCol = \"z2\"\n",
    "val dfnull = Seq(\n",
    "      (1, 1, 11, null, \"a\"),\n",
    "      (2, 1, 22, null, \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (3, 3, 33, \"ord7\", \"b\"),    \n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "val dfnull2 = dfnull.groupBy(col(\"IP\"), col(\"DAY\")).agg(F.sum(col(\"VAL\")).alias(\"DAILY_VAL\"), F.collect_set(col(\"ORD\")).alias(\"DAILY_ORD\")).toDF()\n",
    "print(dfnull2.withColumn(\"z1\", F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2)))\n",
    "      .withColumn(\"z2\", distinctElemCount(col(\"z1\")))\n",
    "      //.withColumn(\"z2\", when($\"z2\"===0, null).otherwise(col(\"z2\")))\n",
    "      .withColumn(tmpCol, when(col(tmpCol)===0, null).otherwise(col(tmpCol)))\n",
    "      .show(10, false))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//round floats: strictfp annotation is one option, bigdecimal has no add operations causing compilation failure, trying round for now\n",
    "\n",
    "val dfFloat = df1.withColumn(\"someFloat1\",F.lit(66232.58783497249767)).withColumn(\"someFloat2\",F.lit(22.187889174987158723497249767))\n",
    "val dfFloat2 = dfFloat.withColumn(\"someFloat1\", F.round($\"someFloat1\", 3)).withColumn(\"DAY\", F.round($\"DAY\", 3))\n",
    "val floatCols: List[String] = List(\"someFloat1\", \"someFloat2\", \"DAY\")\n",
    "\n",
    "val dfFloat3 = floatCols.foldLeft(dfFloat)((acc, c) =>\n",
    "  acc.withColumn(c, F.round(F.col(c), 3))\n",
    ")\n",
    "\n",
    "dfFloat.show()\n",
    "dfFloat.printSchema()\n",
    "\n",
    "dfFloat2.show()\n",
    "dfFloat2.printSchema()\n",
    "\n",
    "dfFloat3.show()\n",
    "dfFloat3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tinker with safedivide function with null values\n",
    "\n",
    "import org.apache.spark.sql.functions.{col, lit, when}\n",
    "import org.apache.spark.sql.{Column, Row}\n",
    "import org.apache.spark.sql.types.{StructType, StructField, LongType, StringType, IntegerType}\n",
    "\n",
    "val schema = List(\n",
    "  StructField(\"v1\", IntegerType, true),\n",
    "  StructField(\"v2\", IntegerType, true)\n",
    ")\n",
    "\n",
    "val data = Seq(\n",
    "  Row(null, 3),\n",
    "  Row(1, 5),\n",
    "  Row(2, 4),\n",
    "  Row(3, null),\n",
    "  Row(null, null),\n",
    "  Row(null, 0),\n",
    "  Row(1, 0)\n",
    ")\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(data),\n",
    "  StructType(schema)\n",
    ")\n",
    "print(df.show())\n",
    "\n",
    "def safeDivideColumns(column1: Column, column2: Column): Column = {\n",
    "    when(column1.isNull, lit(\"\")).otherwise(column1) / when(column2.isNull || column2===0, lit(\"\")).otherwise(column2)\n",
    "}\n",
    "\n",
    "val df2 = df.withColumn(\"v3\", safeDivideColumns(col(\"v1\"), col(\"v2\"))).withColumn(\"v4\", col(\"v3\")+1)\n",
    "print(df2.show())\n",
    "\n",
    "\n",
    "// val w1 = Window.orderBy(\"v3\").rangeBetween(-10, 10)\n",
    "val w1 = Window.orderBy(\"v3\").rowsBetween(-10, 10)\n",
    "val df3 = df2.withColumn(\"v5\", F.sum(\"v3\").over(w1))\n",
    "print(df3.show())\n",
    "\n",
    "// // compute collect set over daily collect set aggs\n",
    "// print(df2.withColumn(\"z1\", F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2))).withColumn(\"z2\", distinctElemCount(col(\"z1\"))).show(10, false))\n",
    "// print(df2.withColumn(\"z1\", distinctElemCount(F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2)))).show())\n",
    "// val df3 = df2.withColumn(\"WINDOWED_VAL\", F.sum(col(\"DAILY_VAL\")).over(w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//// Tinker with null values based on dataframe col type, append df, foldleft df join, filter type check\n",
    "\n",
    "// val schema = StructType(Array(\n",
    "//     StructField(\"DATETIME\", LongType, true),\n",
    "//     StructField(\"DAY\", LongType, true),\n",
    "//     StructField(\"IP\", StringType, true),\n",
    "//     StructField(\"ORD\", StringType, true),\n",
    "//     StructField(\"VAL\", LongType, true)))\n",
    "     \n",
    "// val s = Seq(\n",
    "//       (1, 1, 11, \"ord1\", \"a\"),\n",
    "//       (2, 1, 22, \"ord2\", \"a\"),\n",
    "//       (5, 5, 13, \"ord5\", \"a\"),\n",
    "//       (6, 5, 14, \"ord6\", \"b\"),\n",
    "//       (3, 3, 33, \"ord3\", \"b\"),\n",
    "//       (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "//// null check\n",
    "// df4.schema.fields(0).dataType\n",
    "// df4.schema.fieldIndex(\"IP\")\n",
    "\n",
    "////foldleft df join\n",
    "val df1 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "df1.printSchema()\n",
    "print(df1.filter(col(\"DAY\").isNotNull && !col(\"DAY\").isNaN && $\"DAY\"=!=\"3\").show())\n",
    "println(\"-----\")\n",
    "// Buggy prints below\n",
    "print(df1.filter(col(\"DAY\").isNotNull && !col(\"DAY\").isNaN && $\"DAY\"=!=\"3\" && $\"DAY\"=!=\"\").show())\n",
    "println(\"-----\")\n",
    "// Fixed bugs in above prints\n",
    "val df1_bugfix = df1.withColumn(\"DAY\", col(\"DAY\").cast(StringType))\n",
    "print(df1_bugfix.filter(col(\"DAY\").isNotNull && !col(\"DAY\").isNaN && $\"DAY\"=!=\"3\" && $\"DAY\"=!=\"\").show())\n",
    "println(\"-----\")\n",
    "\n",
    "val df10 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (3, 3, 33, \"ord33\", \"b\"),\n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "val df11 = df1.withColumn(\"C1\", F.lit(1)).withColumn(\"C2\", F.lit(2))\n",
    "val df12 = df1.withColumn(\"C3\", F.lit(3)).withColumn(\"C4\", F.lit(4))\n",
    "val df13 = df1.filter($\"ORD\" =!= \"ord1\" && $\"ORD\" =!= \"ord2\")\n",
    "print(df13.show())\n",
    "\n",
    "val dfs = Seq(df11, df12, df13)\n",
    "val fdf = dfs.foldLeft(df10)(\n",
    "  (acc, df) => acc.join(df, Seq(\"ORD\"), \"left\")\n",
    ")\n",
    "print(fdf.show())\n",
    "// print(fdf.na.fill(1).show())\n",
    "// fdf.agg(F.sum(\"C1\")).show()\n",
    "// df10.filter(col(\"IP\").isNotNull && col(\"IP\") =!= 1).show()\n",
    "\n",
    "////filter type check\n",
    "// df10.filter(col(\"IP\").isNotNull && col(\"IP\") =!= 2).show() //string\n",
    "// df10.filter(col(\"DAY\").isNotNull && col(\"DAY\") =!= \"11.0\").show() //int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tinker with Timstamp\n",
    "\n",
    "//2018-12-01 02:00:00.000|2018-11-16|\n",
    "\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, FloatType, IntegerType, TimestampType, DoubleType, LongType}\n",
    "import org.apache.spark.sql.functions.{col, lit, when}\n",
    "\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"DATETIME\", TimestampType, true),\n",
    "    StructField(\"DAY\", TimestampType, true),\n",
    "    StructField(\"VAL\", LongType, true)))\n",
    "      \n",
    "val someData = Seq(\n",
    "      (\"2018-12-01 02:00:00.000\", \"2018-12-01\", 11),\n",
    "      (\"2019-01-01 02:00:00.000\", \"2019-01-01\", 22),\n",
    "      (\"2019-02-01 02:00:00.000\", \"2019-02-01\", 13),\n",
    "      (\"2019-03-01 02:00:00.000\", \"2019-03-01\", 14),\n",
    "      (\"2019-04-01 02:00:00.000\", \"2019-04-01\", 33),\n",
    "      (\"0\", \"0\", 333),\n",
    "      (\"2019-05-01 02:00:00.000\", \"2019-05-01\", 44))\n",
    "\n",
    "val df = someData.toDF(\"DATETIME\", \"DAY\", \"VAL\")\n",
    "    .withColumn(\"DATETIME\", F.to_timestamp($\"DATETIME\"))\n",
    "    .withColumn(\"DAY\", F.to_timestamp($\"DAY\"))\n",
    "\n",
    "// val someDF = spark.createDataFrame(\n",
    "//   spark.sparkContext.parallelize(someData),\n",
    "//   StructType(schema)\n",
    "// )\n",
    "\n",
    "val dt = F.lit(\"2019-02-02\").cast(\"timestamp\")\n",
    "df.filter(F.col(\"DATETIME\") > dt).show()\n",
    "df.withColumn(\"VAL\", when(F.col(\"DATETIME\") > dt, 1).otherwise(F.col(\"VAL\"))).show()\n",
    "// df.repartition(100,F.col(\"DATETIME\")).persist().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Declare some dummy dataframe\n",
    "\n",
    "val df = Seq(\n",
    "      (1, \"foo\"),\n",
    "      (2, \"barrio\"),\n",
    "      (3, \"gitten\"),\n",
    "      (33, \"NA\"),\n",
    "      (4, \"baa\")).toDF(\"id\", \"words\")\n",
    "\n",
    "// val someData = Seq(\n",
    "//   Row(8, \"bat\"),\n",
    "//   Row(64, \"mouse\"),\n",
    "//   Row(-27, \"horse\")\n",
    "// )\n",
    "\n",
    "// val someSchema = List(\n",
    "//   StructField(\"number\", IntegerType, true),\n",
    "//   StructField(\"word\", StringType, true)\n",
    "// )\n",
    "\n",
    "// val someDF = spark.createDataFrame(\n",
    "//   spark.sparkContext.parallelize(someData),\n",
    "//   StructType(someSchema)\n",
    "// )\n",
    "\n",
    "// // dictionary Set of words to check \n",
    "// val d = Set(\"foo\",\"bar\",\"baa\")\n",
    "\n",
    "// println(s\"Num of partitions:${df.rdd.getNumPartitions},  All partition sizes:[${df.rdd.glom().map(_.length).collect().mkString(\",\")}]\")\n",
    "\n",
    "df.show()\n",
    "df.na.fill(\"\").show()\n",
    "\n",
    "df.withColumn(\"isValid\", $\"words\".isInCollection(d)).show()\n",
    "df.filter($\"words\".isInCollection(d)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Read avro data from s3\n",
    "// https://github.com/julianpeeters/avro-spark-examples/blob/master/src/main/scala/AvroSparkScala.scala\n",
    "\n",
    "// import org.apache.avro.generic.GenericRecord\n",
    "// import org.apache.avro.mapred.AvroKey\n",
    "// import org.apache.avro.mapreduce.AvroKeyInputFormat\n",
    "// import org.apache.hadoop.io.NullWritable\n",
    "// val rdd = sc.newAPIHadoopFile(local_path,\n",
    "//   classOf[AvroKeyInputFormat[GenericRecord]],\n",
    "//   classOf[AvroKey[GenericRecord]],\n",
    "//   classOf[NullWritable]\n",
    "// )\n",
    "// rdd.collect()\n",
    "\n",
    "// s3a://key:secret@\n",
    "val local_path = \"./data/part.avro\"\n",
    "\n",
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"fs.s3a.access.key\", \"\")\n",
    "conf.set(\"fs.s3a.secret.key\", \"\")\n",
    "\n",
    "val df = spark.read\n",
    "    .format(\"com.databricks.spark.avro\")\n",
    "    .load(local_path)\n",
    "df.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
