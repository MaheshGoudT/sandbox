{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker related information\n",
    "\n",
    "https://github.com/jupyter/docker-stacks\n",
    "\n",
    "localnotebookpath='/Users/maheshgoud/Documents/Git_Modules/gh/sandbox/'\n",
    "\n",
    "localdatapath='/Users/maheshgoud/Downloads/data/'\n",
    "\n",
    "#allsparkpackagenames='org.apache.spark:spark-avro_2.11:2.4.0,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2'\n",
    "\n",
    "docker run -it --rm -p 8889:8888 -v $localnotebookpath:/home/jovyan -v $localdatapath:/home/jovyan/data -e JUPYTER_ENABLE_LAB=yes -e SPARK_OPTS='--packages org.apache.spark:spark-avro_2.11:2.4.0,com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2' \\\n",
    "-e PYSPARK_SUBMIT_ARGS='--packages com.databricks:spark-avro_2.10:2.0.1 pyspark-shell' jupyter/all-spark-notebook:7d427e7a4dde start-notebook.sh --NotebookApp.token=''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# build.sbt\n",
    "\n",
    "name := \"app\"\n",
    "version := \"1\"\n",
    "scalaVersion := \"2.11.12\"\n",
    "resolvers += \"Spark Packages Repo\" at \"http://dl.bintray.com/spark-packages/maven\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.4.0\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\"\n",
    "//libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"2.4.0\"\n",
    "//libraryDependencies += \"com.databricks\" %% \"spark-avro\" % \"4.0.0\"\n",
    "//libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.5\" % \"test\"\n",
    "//libraryDependencies += \"com.github.mrpowers\" % \"spark-fast-tests_2.11\" % \"0.12.5\" % \"test\"\n",
    "//libraryDependencies += \"mrpowers\" % \"spark-daria\" % \"2.2.0_0.12.0\" % \"test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful code snippets\n",
    "\n",
    "- https://www.programcreek.com/scala/org.apache.spark.SparkConf (code snippets)\n",
    "- https://gist.github.com/eddies/f37d696567f15b33029277ee9084c4a0 (s3 access, packages related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.Seq\n",
    "import sys.process._\n",
    "\n",
    "// import spark.implicits._\n",
    "// import org.apache.spark.sql.Row\n",
    "\n",
    "println(\"Spark version: \"+sc.version)\n",
    "println(\"Scala version: \"+util.Properties.versionString)\n",
    "\"python --version\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tinker with functional pattern + daily agg window function\n",
    "\n",
    "import org.apache.spark.sql.expressions.{Window, WindowSpec}\n",
    "import org.apache.spark.sql.functions.{col}\n",
    "val F = org.apache.spark.sql.functions\n",
    "\n",
    "val df1 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "print(df1.show())\n",
    "val w = Window.partitionBy(col(\"IP\")).orderBy(\"DAY\").rangeBetween(1, 4)\n",
    "\n",
    "val df2 = df1.groupBy(col(\"IP\"), col(\"DAY\")).agg(F.sum(col(\"VAL\")).alias(\"DAILY_VAL\"), F.collect_set(col(\"ORD\")).alias(\"DAILY_ORD\")).toDF()\n",
    "print(df2.show())\n",
    "\n",
    "// compute collect set over daily collect set aggs\n",
    "val w2 = Window.orderBy(\"DAY\").rangeBetween(-5, 0)\n",
    "// print(df2.withColumn(\"z1\", F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2))).withColumn(\"z2\", F.size(col(\"z1\"))).show())\n",
    "print(df2.withColumn(\"z1\", F.size(F.flatten(F.collect_set(col(\"DAILY_ORD\")).over(w2)))).show())\n",
    "\n",
    "val df3 = df2.withColumn(\"WINDOWED_VAL\", F.sum(col(\"DAILY_VAL\")).over(w))\n",
    "print(df3.show())\n",
    "\n",
    "val df4 = df3.withColumn(\"ORD\", F.explode(col(\"DAILY_ORD\"))).drop(\"DAILY_ORD\")\n",
    "print(df4.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "////Test groupby agg exprs\n",
    "import org.apache.spark.sql.Column\n",
    "\n",
    "// //ver1\n",
    "// println(df1.groupBy(col(\"IP\"), col(\"DAY\")).agg(F.sum(col(\"VAL\")).alias(\"DAILY_VAL\"), F.sum(col(\"DATETIME\")).alias(\"DAILY_DATETIME\")).toDF().show())\n",
    "// val exprs1 = Seq(\"VAL\",\"DATETIME\").map((_ -> \"sum\")).toMap\n",
    "// println(exprs1)\n",
    "// println(df1.groupBy(col(\"IP\"), col(\"DAY\")).agg(exprs1).toDF().show()) \n",
    "\n",
    "// println(\"-\"*50)\n",
    "\n",
    "//ver2\n",
    "val mapping: Map[String, Column => Column] = Map(\"MIN\" -> F.min, \"MAX\" -> F.max, \"MEAN\" -> F.avg, \"SUM\" -> F.sum, \"COUNT\" -> F.count)\n",
    "val operations = Seq(\"MIN\", \"MAX\", \"SUM\", \"COUNT\")\n",
    "val exprs2 = List(\"VAL\",\"DATETIME\").flatMap(c => operations.map(f => mapping(f)(col(c)).alias(s\"DAILY_${f}_${c}\")))\n",
    "println(exprs2)\n",
    "println(df1.groupBy(col(\"IP\"), col(\"DAY\")).agg(exprs2.head, exprs2.tail: _*).show) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//// Tinker with null values based on dataframe col type, append df, foldleft df join, filter type check\n",
    "\n",
    "// val schema = StructType(Array(\n",
    "//     StructField(\"DATETIME\", LongType, true),\n",
    "//     StructField(\"DAY\", LongType, true),\n",
    "//     StructField(\"IP\", StringType, true),\n",
    "//     StructField(\"ORD\", StringType, true),\n",
    "//     StructField(\"VAL\", LongType, true)))\n",
    "     \n",
    "// val s = Seq(\n",
    "//       (1, 1, 11, \"ord1\", \"a\"),\n",
    "//       (2, 1, 22, \"ord2\", \"a\"),\n",
    "//       (5, 5, 13, \"ord5\", \"a\"),\n",
    "//       (6, 5, 14, \"ord6\", \"b\"),\n",
    "//       (3, 3, 33, \"ord3\", \"b\"),\n",
    "//       (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "//// null check\n",
    "// df4.schema.fields(0).dataType\n",
    "// df4.schema.fieldIndex(\"IP\")\n",
    "\n",
    "////foldleft df join\n",
    "val df1 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "val df10 = Seq(\n",
    "      (1, 1, 11, \"ord1\", \"a\"),\n",
    "      (2, 1, 22, \"ord2\", \"a\"),\n",
    "      (5, 5, 13, \"ord5\", \"a\"),\n",
    "      (6, 5, 14, \"ord6\", \"b\"),\n",
    "      (3, 3, 33, \"ord3\", \"b\"),\n",
    "      (3, 3, 33, \"ord33\", \"b\"),\n",
    "      (4, 3, 44, \"ord4\", \"b\")).toDF(\"DATETIME\", \"DAY\", \"VAL\", \"ORD\", \"IP\")\n",
    "\n",
    "val df11 = df1.withColumn(\"C1\", F.lit(1)).withColumn(\"C2\", F.lit(2))\n",
    "val df12 = df1.withColumn(\"C3\", F.lit(3)).withColumn(\"C4\", F.lit(4))\n",
    "val df13 = df1.filter($\"ORD\" =!= \"ord1\" && $\"ORD\" =!= \"ord2\")\n",
    "\n",
    "val dfs = Seq(df11, df12, df13)\n",
    "val fdf = dfs.foldLeft(df10)(\n",
    "  (acc, df) => acc.join(df, Seq(\"ORD\"), \"left\")\n",
    ")\n",
    "print(fdf.show())\n",
    "// print(fdf.na.fill(1).show())\n",
    "// fdf.agg(F.sum(\"C1\")).show()\n",
    "// df10.filter(col(\"IP\").isNotNull && col(\"IP\") =!= 1).show()\n",
    "\n",
    "////filter type check\n",
    "// df10.filter(col(\"IP\").isNotNull && col(\"IP\") =!= 2).show() //string\n",
    "// df10.filter(col(\"DAY\").isNotNull && col(\"DAY\") =!= \"11.0\").show() //int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Declare some dummy dataframe\n",
    "\n",
    "val df = Seq(\n",
    "      (1, \"foo\"),\n",
    "      (2, \"barrio\"),\n",
    "      (3, \"gitten\"),\n",
    "      (4, \"baa\")).toDF(\"id\", \"words\")\n",
    "\n",
    "// val someData = Seq(\n",
    "//   Row(8, \"bat\"),\n",
    "//   Row(64, \"mouse\"),\n",
    "//   Row(-27, \"horse\")\n",
    "// )\n",
    "\n",
    "// val someSchema = List(\n",
    "//   StructField(\"number\", IntegerType, true),\n",
    "//   StructField(\"word\", StringType, true)\n",
    "// )\n",
    "\n",
    "// val someDF = spark.createDataFrame(\n",
    "//   spark.sparkContext.parallelize(someData),\n",
    "//   StructType(someSchema)\n",
    "// )\n",
    "\n",
    "// dictionary Set of words to check \n",
    "val d = Set(\"foo\",\"bar\",\"baa\")\n",
    "\n",
    "println(s\"Num of partitions:${df.rdd.getNumPartitions},  All partition sizes:[${df.rdd.glom().map(_.length).collect().mkString(\",\")}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isValid\", $\"words\".isInCollection(d)).show()\n",
    "df.filter($\"words\".isInCollection(d)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Read avro data from s3\n",
    "// https://github.com/julianpeeters/avro-spark-examples/blob/master/src/main/scala/AvroSparkScala.scala\n",
    "\n",
    "// import org.apache.avro.generic.GenericRecord\n",
    "// import org.apache.avro.mapred.AvroKey\n",
    "// import org.apache.avro.mapreduce.AvroKeyInputFormat\n",
    "// import org.apache.hadoop.io.NullWritable\n",
    "// val rdd = sc.newAPIHadoopFile(local_path,\n",
    "//   classOf[AvroKeyInputFormat[GenericRecord]],\n",
    "//   classOf[AvroKey[GenericRecord]],\n",
    "//   classOf[NullWritable]\n",
    "// )\n",
    "// rdd.collect()\n",
    "\n",
    "// s3a://key:secret@\n",
    "val local_path = \"./data/part.avro\"\n",
    "\n",
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"fs.s3a.access.key\", \"\")\n",
    "conf.set(\"fs.s3a.secret.key\", \"\")\n",
    "\n",
    "val df = spark.read\n",
    "    .format(\"com.databricks.spark.avro\")\n",
    "    .load(local_path)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
