{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# np.zeros((5,4))\n",
    "# np.random.randint(1,7,(5,4))\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(3, 4)), columns=list('ABCD'))\n",
    "df.iloc[0] = [np.nan, np.nan, np.nan, 'nan']\n",
    "print(df.isnull().sum().sum())\n",
    "display(df)\n",
    "\n",
    "df2 = df[['A','B']]\n",
    "df2['E'] = df2['A']+1\n",
    "display(df2)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, pd.DataFrame(np.array([1,2,3]))], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([1,2,3])\n",
    "c[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "-sys.maxsize\n",
    "\n",
    "#nan based assert\n",
    "a, b = np.nan, np.nan\n",
    "a, b = 1,1\n",
    "# a, b = 1,np.nan\n",
    "assert ((np.isnan(a) and np.isnan(b)) or (a == b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datetime functions\n",
    "\n",
    "# import pandas as pd\n",
    "# import datetime\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# class Const:\n",
    "#     cur_date = datetime.datetime.today()\n",
    "#     prior_date = (datetime.datetime.today() + relativedelta(months=-1))\n",
    "#     date_ranges = pd.date_range(start=prior_date.strftime(\"%m/%d/%Y\"), end=cur_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "import io\n",
    "data = {'date':['03/28/2019  10:08:44 PM','4/26/2019  12:43:07 AM','4/35/2019  12:43:07 AM','',0,'0']}\n",
    "df = pd.DataFrame(data)\n",
    "# df = pd.read_csv(io.StringIO(data))\n",
    "# encoding='latin1')\n",
    "\n",
    "df['f_date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "max_date = max(df['f_date'])\n",
    "date_val = max_date - pd.to_timedelta(50, 'D') \n",
    "df['flag'] = (df['f_date'] >= date_val)\n",
    "\n",
    "print(df)\n",
    "print(max_date, date_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utc time now\n",
    "\n",
    "print(pd.to_datetime('now').strftime('UTC-%Y-%m-%d %H:%M:%S'))\n",
    "print(datetime.datetime.utcnow().strftime('UTC-%Y-%m-%d %H:%M:%S'))\n",
    "print(datetime.datetime.now(pytz.utc).strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.journaldev.com/15631/python-multiprocessing-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PS1='\\!:\\[\\033[0;32m\\]$( basename `git rev-parse --show-toplevel 2> /dev/null` 2> /dev/null | sed \"s/^/[/\")$( git branch 2> /dev/null | cut -f2 -d\\* -s | sed \"s/^ /:/\" | sed \"s/$/]/\" )\\[\\033[0m\\]$( pwd | sed \"s|`git rev-parse --show-toplevel 2> /dev/null`||\" 2> /dev/null || dirs +0) \\$ '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read many part files from AWS s3\n",
    "\n",
    "import boto3, io, s3fs\n",
    "from multiprocessing import Pool\n",
    "\n",
    "## Connections\n",
    "\n",
    "# conn = connect(\n",
    "#     aws_access_key_id='YOUR_ACCESS_KEY_ID',\n",
    "#     aws_secret_access_key='YOUR_SECRET_ACCESS_KEY',\n",
    "#     s3_staging_dir='s3://bucket/staging/',\n",
    "#     region_name='r1')\n",
    "\n",
    "# cursor = connect(\n",
    "#     aws_access_key_id='YOUR_ACCESS_KEY_ID',\n",
    "#     aws_secret_access_key='YOUR_SECRET_ACCESS_KEY',\n",
    "#     s3_staging_dir='s3://bucket/staging/',\n",
    "#     region_name='r1').cursor()\n",
    "\n",
    "## --------------------------------------------------\n",
    "## Pandas batch style based data fetch (Error: Query times out for large datasets)\n",
    "\n",
    "# def read_data_from_athena(db_name, table_name):\n",
    "#     query = \"SELECT * FROM {}.{}\".format(db_name, table_name) \n",
    "#     cur_df = pd.read_sql(query, conn)\n",
    "#     return cur_df\n",
    "\n",
    "# raw_training_df = read_data_from_athena(db_name, table_name)\n",
    "\n",
    "## --------------------------------------------------\n",
    "## Cursor based data fetch (Error: No result set for large datasets)\n",
    "\n",
    "# def get_cursor_iterator(db_name, table_name):\n",
    "#     query = \"SELECT * FROM {}.{} limit 10\".format(db_name, table_name)\n",
    "#     print(query)\n",
    "#     for row in cursor:\n",
    "#         print(row)\n",
    "#         break\n",
    "\n",
    "# get_cursor_iterator(db_name, table_name)\n",
    "\n",
    "## --------------------------------------------------\n",
    "def serial_read_part_files_to_df(bucket_name, parent_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    prefix_objs = bucket.objects.filter(Prefix=parent_key)\n",
    "    sub_df = []\n",
    "    for obj in prefix_objs:\n",
    "        file_key = obj.key\n",
    "        print(file_key)\n",
    "        cur_df = pd.read_csv('s3://'+bucket_name+'/'+file_key, compression='gzip', header=0, sep='|', quotechar='\"')\n",
    "        cur_df = cur_df[valid_cols]\n",
    "        sub_df.append(cur_df)\n",
    "    return pd.concat(sub_df)\n",
    "\n",
    "def get_data(bucket_name, file_key, valid_cols):\n",
    "    print(file_key)\n",
    "    cur_df = pd.read_csv('s3://'+bucket_name+'/'+file_key, compression='gzip', header=0, sep='|', quotechar='\"')\n",
    "    cur_df = cur_df[valid_cols]\n",
    "    return cur_df\n",
    "\n",
    "class GET_DATA(): #Helper class to invoke get_data() as local lambda's cannot be pickled so cannot be used with pool.map. functools.partial can also be used \n",
    "    def __init__(self, bucket_name, valid_cols):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.valid_cols = valid_cols\n",
    "    def __call__(self, file_key):\n",
    "        return get_data(self.bucket_name, file_key, self.valid_cols)\n",
    "\n",
    "def parallel_read_part_files_to_df(bucket_name, parent_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    prefix_objs = bucket.objects.filter(Prefix=parent_key)\n",
    "    file_keys = tuple(map(lambda x : x.key, prefix_objs))\n",
    "    agents, chunksize = 5, 25\n",
    "    GD = GET_DATA(bucket_name, valid_cols)\n",
    "    with Pool(processes=agents) as pool:\n",
    "        sub_df = pool.map(GD, file_keys, chunksize)\n",
    "    return pd.concat(sub_df)\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def read_df_from_s3(file_path, header_flag=True):\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "    if header_flag:\n",
    "        tmp_df = pd.read_csv(BytesIO(obj['Body'].read()))\n",
    "    else:\n",
    "        tmp_df = pd.read_csv(BytesIO(obj['Body'].read()), header=None)\n",
    "    return tmp_df\n",
    "\n",
    "def write_df_to_s3(cur_df, bucket_name, file_key):\n",
    "    csv_buffer = StringIO()\n",
    "    cur_df.to_csv(csv_buffer, index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket_name, file_key).put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS cli commands\n",
    "\n",
    "aws batch submit-job --generate-cli-skeleton\n",
    "aws lambda update-function-code --function-name \"abc\" --s3-bucket \"def\" --s3-key \"code/ghi.zip\"\n",
    "aws ec2 describe-vpcs\n",
    "aws ec2 describe-nat-gateways\n",
    "\n",
    "aws emr create-cluster \\\n",
    "  --release-label emr-5.25.0 \\\n",
    "  --name 'abc' \\\n",
    "  --applications Name=Hadoop Name=Hive Name=Spark Name=Ganglia \\\n",
    "  --tags 'def=ghi' \\\n",
    "  --ec2-attributes KeyName=k1,SubnetId=s1,EmrManagedMasterSecurityGroup=s2,EmrManagedSlaveSecurityGroup=s3,ServiceAccessSecurityGroup=s4,InstanceProfile=p1 \\\n",
    "  --service-role EMR_DefaultRole --enable-debugging \\\n",
    "  --instance-groups \\\n",
    "    InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m5.2xlarge \\\n",
    "    InstanceGroupType=CORE,InstanceCount=1,InstanceType=r5.12xlarge,BidPrice=b \\\n",
    "  --region r1 \\\n",
    "  --log-uri s2://u1\n",
    "\n",
    "aws emr add-steps --cluster-id abc --steps file://def.json\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script ec2 creation\n",
    "\n",
    "import boto3\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "ec2 = boto3.resource('ec2')\n",
    "REGION = os.getenv('region', 'r1')\n",
    "ENV = os.getenv('env', 'defaultenv')\n",
    "iam_instance_profile = os.getenv('iam_instance_profile', '')\n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    response = ec2.create_instances(\n",
    "        BlockDeviceMappings=[\n",
    "            {\n",
    "                'DeviceName': '/dev/sda1', #EBS-optimized has become False due to this change\n",
    "                'Ebs': {\n",
    "                    'DeleteOnTermination': True,\n",
    "                    'VolumeSize': 50\n",
    "                    #'VolumeType': 'gp2'\n",
    "                }\n",
    "            },\n",
    "        ],        \n",
    "        ImageId='ami-....ubuntu',\n",
    "        InstanceType='',\n",
    "        KeyName='',\n",
    "        MaxCount=1,\n",
    "        MinCount=1,\n",
    "        Monitoring={\n",
    "            'Enabled': True\n",
    "        },\n",
    "        SecurityGroupIds=[\n",
    "            '',\n",
    "        ],\n",
    "        SubnetId='',\n",
    "        IamInstanceProfile={\n",
    "            'Arn': iam_instance_profile,\n",
    "        },\n",
    "        InstanceInitiatedShutdownBehavior='terminate',\n",
    "        TagSpecifications=[\n",
    "            {\n",
    "                'ResourceType': 'instance',\n",
    "                'Tags': [\n",
    "                    {\n",
    "                        'Key': 'k1',\n",
    "                        'Value': v1\n",
    "                    },\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    print(response)\n",
    "    try:\n",
    "        if len(response) > 0:\n",
    "            return {'status': 'OK', 'instance_id': response[0].id,\n",
    "                    'ResponseMetadata': {'response_timestamp': int(time.time()),\n",
    "                                         'response_datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}}\n",
    "    except:\n",
    "        print('Error occurred during instance creation...')\n",
    "    return {'status': 'ERROR', 'error_message': 'Error occurred while creating EC2 instance.',\n",
    "            'ResponseMetadata': {'response_timestamp': int(time.time()),\n",
    "                                 'response_datetime': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    handler(\"\",\"\")\n",
    "\n",
    "# Start jupyter lab without authentication    \n",
    "jupyter lab --ip=* --NotebookApp.token=''    \n",
    "\n",
    "# Within aws lambda\n",
    "os.system(\"unzip -x /tmp/amlbins.zip -d /tmp/amlbins >/dev/null 2>&1\") #NOTE: redirecting stdout, stderr to null so as to easily debug lambda logs on cloudwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Publish metrics to cloudwatch metrics\n",
    "# https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\n",
    "\n",
    "d = {1:2, 3:4, 5:6}\n",
    "\n",
    "import boto3\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "def is_potential_float(v):\n",
    "    try:\n",
    "        float(v)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "#Rather than differentiating dev/prod metrics by dimensions lets push dev/prod metrics to different namespaces\n",
    "def iterate_dict(d, cur_metric_data):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            iterate_dict(v, cur_metric_data)\n",
    "        elif is_potential_float(v):\n",
    "            v = float(v)\n",
    "            cur_metric_data.append(\n",
    "                {\n",
    "                    'MetricName': k,\n",
    "                    'Unit': 'None',\n",
    "                    'Value': v\n",
    "                }\n",
    "            )\n",
    "\n",
    "def publish_cw_metricdata_from_dict(d):\n",
    "    cur_metric_data = []\n",
    "    cur_namespace = 'a/b'\n",
    "    iterate_dict(d, cur_metric_data)\n",
    "    #print(cur_metric_data)\n",
    "    response = cloudwatch.put_metric_data(\n",
    "        MetricData = cur_metric_data,\n",
    "        Namespace = cur_namespace\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = publish_cw_metricdata_from_dict(d)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Publish metrics to Prometheus\n",
    "# https://github.com/prometheus/client_python\n",
    "\n",
    "##Publish metrics to elastic search\n",
    "# https://tryolabs.com/blog/2015/02/17/python-elasticsearch-first-steps/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//Amazon lambda docker\n",
    "FROM amazonlinux:latest\n",
    "MAINTAINER Mahesh \"abcd@gmail.com\"\n",
    "\n",
    "ADD requirements.txt /home/requirements.txt\n",
    "WORKDIR /home\n",
    "\n",
    "RUN yum -y install python3 findutils wget zip unzip file && \\\n",
    "\tpython3 -m venv amlenv && \\\n",
    "\tsource amlenv/bin/activate && \\\n",
    "\tamlenv/bin/pip3 install -t amlbins/ -r requirements.txt\n",
    "\n",
    "\n",
    "//Minimal docker\n",
    "FROM alpine:3.8\n",
    "WORKDIR /usr/src/app\n",
    "COPY . .\n",
    "\n",
    "\n",
    "//emr steps.json\n",
    "[\n",
    "  {\n",
    "    \"Name\": \"Install Docker, Start Docker, Login To ECR\",\n",
    "    \"Args\": [\"bash\",\"-c\",\n",
    "      \"sudo yum -y install docker && sudo service docker start && sudo $(aws ecr get-login --no-include-email --region r1)\"\n",
    "    ],\n",
    "    \"Jar\": \"command-runner.jar\",\n",
    "    \"ActionOnFailure\": \"CANCEL_AND_WAIT\"\n",
    "  },\n",
    "  {\n",
    "    \"Name\": \"Fetch Jar From Docker\",\n",
    "    \"Args\": [\"bash\",\"-c\",\n",
    "      \"sudo docker run --rm -v /home/hadoop/:/tmp a:b sh -c \\\"cp -r /usr/src/app/* /tmp\\\"\"\n",
    "    ],\n",
    "    \"Jar\": \"command-runner.jar\",\n",
    "    \"ActionOnFailure\": \"CANCEL_AND_WAIT\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker build -t abcd .\n",
    "docker run --rm -v $(pwd):/tmp abcd:latest sh -c \"cp -r /home/amlbins /tmp\"\n",
    "\n",
    "https://github.com/indeedeng/vowpal-wabbit-java/blob/master/build-jni/vw-linux-build-docker-img/Dockerfile\n",
    "\n",
    "[Docker stacks for python]\n",
    "docker run -it --rm -v YOUR_FOLDER_WITH_NOTEBOOKS:/home/jovyan/work -v YOUR_FOLDER_WITH_DATA:/home/jovyan/work/data -p 8888:8888 jupyter/all-spark-notebook\n",
    "folders has to have absolute path\n",
    "\n",
    "[Run bash inside docker within ecr]\n",
    "export AWS_ACCESS_KEY_ID=\"\";\n",
    "export AWS_SECRET_ACCESS_KEY=\"\";\n",
    "gitlab-runner exec shell build\n",
    "sample_imagename=\"python:3.7.2\"\n",
    "docker run -it --rm --entrypoint \"\" $sample_imagename /bin/bash\n",
    "docker run -it --rm -v $(pwd):/tmp --entrypoint \"\" $sample_imagename /bin/bash\n",
    "\n",
    "//To avoid auth issues while pulling/pushing docker to ECR\n",
    "$(aws ecr get-login --no-include-email --region ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract docker tag, digest from AWS ECR\n",
    "\n",
    "import boto3\n",
    "import docker\n",
    "import json\n",
    "\n",
    "repository_name = 'repoparent/reponame'\n",
    "session = boto3.Session(profile_name='default')\n",
    "client = boto3.client('ecr')\n",
    "# docker_api = docker.APIClient()\n",
    "print(session)\n",
    "\n",
    "response = client.batch_get_image(\n",
    "    repositoryName=repository_name,\n",
    "    imageIds=[\n",
    "        {\n",
    "            'imageTag': ''\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# response = client.batch_get_image(\n",
    "#     repositoryName=repository_name,\n",
    "#     imageIds=[\n",
    "#         {\n",
    "#             'imageDigest': 'sha256:...'\n",
    "#         },\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# response = client.list_images(\n",
    "#     repositoryName=repository_name,\n",
    "#     maxResults=5,\n",
    "#     filter={\n",
    "#         'tagStatus': 'TAGGED'\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# from IPython.display import JSON\n",
    "# JSON(response, expanded=True)\n",
    "\n",
    "# import hashlib\n",
    "# print(hashlib.sha256(b\"abc123\").hexdigest())\n",
    "\n",
    "# print(response['images'][0]['imageId']['imageDigest'])\n",
    "print(json.dumps(response,indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 2 tag names for the `recent` env specific docker tag having same image digest\n",
    "\n",
    "env_name = 'non-prod'\n",
    "response = client.batch_get_image(\n",
    "    repositoryName=repository_name,\n",
    "    imageIds=[\n",
    "        {\n",
    "            'imageTag': env_name\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "assert len(response['images']) == 1, \"1 recent docker is expected with tag {} under {} ecr repository\".format(env_name, repository_name)\n",
    "image_digest = response['images'][0]['imageId']['imageDigest']\n",
    "\n",
    "response = client.batch_get_image(\n",
    "    repositoryName=repository_name,\n",
    "    imageIds=[\n",
    "        {\n",
    "            'imageDigest': image_digest\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "assert len(response['images']) == 2, \"2 tags are expected for docker with image_digest {} under {} ecr repository\".format(image_digest, repository_name)\n",
    "tag_digest_list = [(response['images'][idx]['imageId']['imageTag'], response['images'][idx]['imageId']['imageDigest']) \n",
    "                   for idx in range(2)]\n",
    "print(tag_digest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Kafka AvroConsumer message read]\n",
    "\n",
    "docker run --rm confluentinc/cp-kafka /usr/bin/kafka-console-consumer --bootstrap-server <broker:9092> --topic <topic> --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property print.key=true --property key.separator='|'\n",
    "\n",
    "docker run --rm confluentinc/cp-schema-registry /usr/bin/kafka-avro-console-consumer --bootstrap-server <broker:9092> --topic <topic> --property schema.registry.url=http://<registry>:8081 --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property print.key=true --property key.separator='|'\n",
    "\n",
    "--offset 200000 --partition 0 \n",
    "\n",
    "docker run -it --rm confluentinc/cp-ksql-cli:5.3.0 http://ks.cf.abc:8088/\n",
    "\n",
    "kafkacat -b <> -t <> -L #cluster state\n",
    "kafkacat -b <> -t <> -J | grep \"\" #prints offset and key\n",
    "kafkacat -f '%T\\t%S\\n' #look at timestamps and sizes on the existing topic for traffic estimates\n",
    "\n",
    "jq < stuff.txt | grep a | grep -v b | cut -f 1 -d : | jq -r | sort | uniq -c | sort -nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro Message"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Flow:\n",
    "1. do changes to schema\n",
    "2. schema validation for compatibility\n",
    "3. register new schema version\n",
    "4. add new avro stubs to the project\n",
    "5. start publishing objects with new schema to kafka\n",
    "\n",
    "## Delete all compiled code\n",
    "clean:\n",
    "\tmvn clean\n",
    "\n",
    "## Build packages\n",
    "build: clean\n",
    "\tmvn compile\n",
    "\tmvn exec:java\n",
    "\n",
    "## Validate schema against dev and prod topics\n",
    "validate: build\n",
    "\tmvn schema-registry:test-compatibility -P ${ENV}\n",
    "\n",
    "register: validate\n",
    "\tmvn schema-registry:register -P ${ENV}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gitlab CI/CD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test:\n",
    "  stage: test\n",
    "  image: python:3.7.2\n",
    "  before_script:\n",
    "    - pip install virtualenv\n",
    "    - virtualenv venv\n",
    "    - source venv/bin/activate\n",
    "  script:\n",
    "    - pip install -r requirements.txt\n",
    "    - pip install pylint \n",
    "    - pylint -d C,W,R *.py **/*.py\n",
    "  tags:\n",
    "    - abcd cicd build\n",
    "    \n",
    "\n",
    "stages:\n",
    "  - build\n",
    "  - release\n",
    "\n",
    "build:\n",
    "  stage: build\n",
    "  image: mozilla/sbt\n",
    "  #cache:\n",
    "  #  paths:\n",
    "  #    - ~/.ivy2/\n",
    "  script:\n",
    "    - sbt package\n",
    "  artifacts:\n",
    "    expire_in: 1 day\n",
    "    paths:\n",
    "      - target/\n",
    "  tags:\n",
    "    - a\n",
    "    - b\n",
    "    \n",
    ".release: &release\n",
    "  image: docker:stable\n",
    "  before_script:\n",
    "    - apk add python3\n",
    "    - pip3 install awscli\n",
    "    - $(aws ecr get-login --no-include-email --region r1)\n",
    "  script:\n",
    "    - docker build -t ${img} .\n",
    "    - docker tag ${img}:latest ${repo}/${img}:${env}\n",
    "    - docker tag ${img}:latest ${repo}/${img}:${CI_COMMIT_SHA}\n",
    "    - docker push ${repo}/${img}:${env}\n",
    "    - docker push ${repo}/${img}:${CI_COMMIT_SHA}\n",
    "  tags:\n",
    "    - a\n",
    "    - b\n",
    "\n",
    "release:non-prod:\n",
    "  stage: release\n",
    "  variables:\n",
    "    env: non-prod\n",
    "    img: a/b\n",
    "    repo: c\n",
    "  <<: *release\n",
    "  except:\n",
    "    - develop\n",
    "    - master\n",
    "    \n",
    "build:\n",
    "  stage: build\n",
    "  image: python:3.7\n",
    "  script:\n",
    "    - sh build.sh\n",
    "  artifacts:\n",
    "    expire_in: 2 hrs\n",
    "    paths: \n",
    "      - target/\n",
    "  tags:\n",
    "    - a b c\n",
    "\n",
    "deploy:\n",
    "  stage: deploy\n",
    "  image:\n",
    "    name: hashicorp/terraform:0.11.11\n",
    "    entrypoint:\n",
    "      - /usr/bin/env\n",
    "  script:\n",
    "    - cd terraform\n",
    "    - sh init.sh\n",
    "    - terraform apply -auto-approve\n",
    "  dependencies:\n",
    "    - build\n",
    "  tags:\n",
    "    - a \n",
    "    - b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VirtualEnv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!/bin/bash\n",
    "\n",
    "#Mac\n",
    "pip install virtualenv\n",
    "python3 -m virtualenv env\n",
    "\n",
    "#Linux\n",
    "virtualenv -p python3 --no-site-packages venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "#Within requirements.txt have\n",
    "keras\n",
    "tensorflow\n",
    "matplotlib\n",
    "pandas\n",
    "numpy\n",
    "sklearn\n",
    "jupyter\n",
    "seaborn\n",
    "boto3\n",
    "s3fs\n",
    "\n",
    "#python -m easy_install pyyaml\n",
    "\n",
    "#pip install -r target/lambda/requirements.txt -t target/lambda\n",
    "\n",
    "#Conda kernel\n",
    "source ~/anaconda3/etc/profile.d/conda.sh\n",
    "conda create -n py38 python=3.8\n",
    "conda activate py38\n",
    "conda install ipykernel\n",
    "python -m ipykernel install --name py38"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
