{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# np.zeros((5,4))\n",
    "# np.random.randint(1,7,(5,4))\n",
    "df = pd.DataFrame(np.random.randint(0,100,size=(3, 4)), columns=list('ABCD'))\n",
    "df.iloc[0] = [np.nan, np.nan, np.nan, 'nan']\n",
    "print(df.isnull().sum().sum())\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df, pd.DataFrame(np.array([1,2,3]))], axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter([1,2,3])\n",
    "c[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "-sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datetime functions\n",
    "\n",
    "# import pandas as pd\n",
    "# import datetime\n",
    "# from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# class Const:\n",
    "#     cur_date = datetime.datetime.today()\n",
    "#     prior_date = (datetime.datetime.today() + relativedelta(months=-1))\n",
    "#     date_ranges = pd.date_range(start=prior_date.strftime(\"%m/%d/%Y\"), end=cur_date.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "import io\n",
    "data = {'date':['03/28/2019  10:08:44 PM','4/26/2019  12:43:07 AM','4/35/2019  12:43:07 AM','',0,'0']}\n",
    "df = pd.DataFrame(data)\n",
    "# df = pd.read_csv(io.StringIO(data))\n",
    "# encoding='latin1')\n",
    "\n",
    "df['f_date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "max_date = max(df['f_date'])\n",
    "date_val = max_date - pd.to_timedelta(50, 'D') \n",
    "df['flag'] = (df['f_date'] >= date_val)\n",
    "\n",
    "print(df)\n",
    "print(max_date, date_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.journaldev.com/15631/python-multiprocessing-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export PS1='\\!:\\[\\033[0;32m\\]$( basename `git rev-parse --show-toplevel 2> /dev/null` 2> /dev/null | sed \"s/^/[/\")$( git branch 2> /dev/null | cut -f2 -d\\* -s | sed \"s/^ /:/\" | sed \"s/$/]/\" )\\[\\033[0m\\]$( pwd | sed \"s|`git rev-parse --show-toplevel 2> /dev/null`||\" 2> /dev/null || dirs +0) \\$ '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read many part files from AWS s3\n",
    "\n",
    "import boto3, io, s3fs\n",
    "from multiprocessing import Pool\n",
    "\n",
    "## Connections\n",
    "\n",
    "# conn = connect(\n",
    "#     aws_access_key_id='YOUR_ACCESS_KEY_ID',\n",
    "#     aws_secret_access_key='YOUR_SECRET_ACCESS_KEY',\n",
    "#     s3_staging_dir='s3://bucket/staging/',\n",
    "#     region_name='us-east-1')\n",
    "\n",
    "# cursor = connect(\n",
    "#     aws_access_key_id='YOUR_ACCESS_KEY_ID',\n",
    "#     aws_secret_access_key='YOUR_SECRET_ACCESS_KEY',\n",
    "#     s3_staging_dir='s3://bucket/staging/',\n",
    "#     region_name='us-east-1').cursor()\n",
    "\n",
    "## --------------------------------------------------\n",
    "## Pandas batch style based data fetch (Error: Query times out for large datasets)\n",
    "\n",
    "# def read_data_from_athena(db_name, table_name):\n",
    "#     query = \"SELECT * FROM {}.{}\".format(db_name, table_name) \n",
    "#     cur_df = pd.read_sql(query, conn)\n",
    "#     return cur_df\n",
    "\n",
    "# raw_training_df = read_data_from_athena(db_name, table_name)\n",
    "\n",
    "## --------------------------------------------------\n",
    "## Cursor based data fetch (Error: No result set for large datasets)\n",
    "\n",
    "# def get_cursor_iterator(db_name, table_name):\n",
    "#     query = \"SELECT * FROM {}.{} limit 10\".format(db_name, table_name)\n",
    "#     print(query)\n",
    "#     for row in cursor:\n",
    "#         print(row)\n",
    "#         break\n",
    "\n",
    "# get_cursor_iterator(db_name, table_name)\n",
    "\n",
    "## --------------------------------------------------\n",
    "def serial_read_part_files_to_df(bucket_name, parent_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    prefix_objs = bucket.objects.filter(Prefix=parent_key)\n",
    "    sub_df = []\n",
    "    for obj in prefix_objs:\n",
    "        file_key = obj.key\n",
    "        print(file_key)\n",
    "        cur_df = pd.read_csv('s3://'+bucket_name+'/'+file_key, compression='gzip', header=0, sep='|', quotechar='\"')\n",
    "        cur_df = cur_df[valid_cols]\n",
    "        sub_df.append(cur_df)\n",
    "    return pd.concat(sub_df)\n",
    "\n",
    "def get_data(bucket_name, file_key, valid_cols):\n",
    "    print(file_key)\n",
    "    cur_df = pd.read_csv('s3://'+bucket_name+'/'+file_key, compression='gzip', header=0, sep='|', quotechar='\"')\n",
    "    cur_df = cur_df[valid_cols]\n",
    "    return cur_df\n",
    "\n",
    "class GET_DATA(): #Helper class to invoke get_data() as local lambda's cannot be pickled so cannot be used with pool.map. functools.partial can also be used \n",
    "    def __init__(self, bucket_name, valid_cols):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.valid_cols = valid_cols\n",
    "    def __call__(self, file_key):\n",
    "        return get_data(self.bucket_name, file_key, self.valid_cols)\n",
    "\n",
    "def parallel_read_part_files_to_df(bucket_name, parent_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    prefix_objs = bucket.objects.filter(Prefix=parent_key)\n",
    "    file_keys = tuple(map(lambda x : x.key, prefix_objs))\n",
    "    agents, chunksize = 5, 25\n",
    "    GD = GET_DATA(bucket_name, valid_cols)\n",
    "    with Pool(processes=agents) as pool:\n",
    "        sub_df = pool.map(GD, file_keys, chunksize)\n",
    "    return pd.concat(sub_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Publish metrics to cloudwatch metrics\n",
    "# https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\n",
    "\n",
    "d = {1:2, 3:4, 5:6}\n",
    "\n",
    "import boto3\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "def is_potential_float(v):\n",
    "    try:\n",
    "        float(v)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "#Rather than differentiating dev/prod metrics by dimensions lets push dev/prod metrics to different namespaces\n",
    "def iterate_dict(d, cur_metric_data):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            iterate_dict(v, cur_metric_data)\n",
    "        elif is_potential_float(v):\n",
    "            v = float(v)\n",
    "            cur_metric_data.append(\n",
    "                {\n",
    "                    'MetricName': k,\n",
    "                    'Unit': 'None',\n",
    "                    'Value': v\n",
    "                }\n",
    "            )\n",
    "\n",
    "def publish_cw_metricdata_from_dict(d):\n",
    "    cur_metric_data = []\n",
    "    cur_namespace = 'a/b'\n",
    "    iterate_dict(d, cur_metric_data)\n",
    "    #print(cur_metric_data)\n",
    "    response = cloudwatch.put_metric_data(\n",
    "        MetricData = cur_metric_data,\n",
    "        Namespace = cur_namespace\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response = publish_cw_metricdata_from_dict(d)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Publish metrics to Prometheus\n",
    "# https://github.com/prometheus/client_python\n",
    "\n",
    "##Publish metrics to elastic search\n",
    "# https://tryolabs.com/blog/2015/02/17/python-elasticsearch-first-steps/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FROM amazonlinux:latest\n",
    "MAINTAINER Mahesh \"abcd@gmail.com\"\n",
    "\n",
    "ADD requirements.txt /home/requirements.txt\n",
    "WORKDIR /home\n",
    "\n",
    "RUN yum -y install python36 python36-virtualenv python36-pip findutils wget zip unzip file && \\\n",
    "\tpython3 -m venv amlenv && \\\n",
    "\tsource amlenv/bin/activate && \\\n",
    "\tamlenv/bin/pip3 install -t amlbins/ -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "docker build -t abcd .\n",
    "docker run --rm -v $(pwd):/tmp abcd:latest sh -c \"cp -r /home/amlbins /tmp\"\n",
    "\n",
    "https://github.com/indeedeng/vowpal-wabbit-java/blob/master/build-jni/vw-linux-build-docker-img/Dockerfile\n",
    "\n",
    "[Docker stacks for python]\n",
    "docker run -it --rm -v YOUR_FOLDER_WITH_NOTEBOOKS:/home/jovyan/work -v YOUR_FOLDER_WITH_DATA:/home/jovyan/work/data -p 8888:8888 jupyter/all-spark-notebook\n",
    "folders has to have absolute path\n",
    "\n",
    "[Run bash inside docker within ecr]\n",
    "export AWS_ACCESS_KEY_ID=\"\";\n",
    "export AWS_SECRET_ACCESS_KEY=\"\";\n",
    "gitlab-runner exec shell build\n",
    "sample_imagename=\"\" \n",
    "docker run -it --rm --entrypoint \"\" $sample_imagename /bin/bash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gitlab CI/CD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test:\n",
    "  stage: test\n",
    "  image: python:3.7.2\n",
    "  before_script:\n",
    "    - pip install virtualenv\n",
    "    - virtualenv venv\n",
    "    - source venv/bin/activate\n",
    "  script:\n",
    "    - pip install -r requirements.txt\n",
    "    - pip install pylint \n",
    "    - pylint -d C,W,R *.py **/*.py\n",
    "  tags:\n",
    "    - abcd cicd build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VirtualEnv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!/bin/bash\n",
    "\n",
    "#Mac\n",
    "pip install virtualenv\n",
    "python3 -m virtualenv env\n",
    "\n",
    "#Linux\n",
    "virtualenv -p python3 --no-site-packages venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "#Within requirements.txt have\n",
    "keras\n",
    "tensorflow\n",
    "matplotlib\n",
    "pandas\n",
    "numpy\n",
    "sklearn\n",
    "jupyter\n",
    "seaborn\n",
    "boto3\n",
    "s3fs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
